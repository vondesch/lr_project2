desired velocity 0.8
control method CPG
gait mode walk
learning algo PPO

reward = 2*vel_tracking_reward \
            + 2*yaw_reward \
            + drift_reward \
            - 0.01 * energy_reward \
            - 0.1 * np.linalg.norm(self.robot.GetBaseOrientation() - np.array([0,0,0,1]))

observation_high = (np.concatenate((np.array([ 0.261799,  1.5708, -0.916297857297 ] * self._robot_config.NUM_LEGS), # joint limit
                                         self._robot_config.VELOCITY_LIMITS,
                                         np.array([ 2 ] * self._robot_config.NUM_LEGS),
                                         np.array([ 2*np.pi ] * self._robot_config.NUM_LEGS),
                                         np.array([1.0]*4))) +  OBSERVATION_EPS)
observation_low = (np.concatenate((np.array([ -0.261799,  0.261799, -2.69653369433 ] * self._robot_config.NUM_LEGS), # joint limit
                                         -self._robot_config.VELOCITY_LIMITS/1.5,
                                         np.array([ 0, ] * self._robot_config.NUM_LEGS),
                                         np.array([ 0 ] * self._robot_config.NUM_LEGS),
                                         np.array([-1.0]*4))) -  OBSERVATION_EPS)

it walks with some struggle, but straight, with the correct speed
best try Final base position (7.872869394424687, 0.1106547197153421, 0.3215295843718514)
take this one for the forward report model.