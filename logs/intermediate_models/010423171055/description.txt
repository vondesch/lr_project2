gait walk
learning alg PPO
desired velocity 0.6
motor control PD

observation_high = (np.concatenate((np.array([ 0.261799,  1.5708, -0.916297857297 ] * self._robot_config.NUM_LEGS), # joint limit
                                          self._robot_config.VELOCITY_LIMITS, # limit on velocity
                                          np.array([1]* self._robot_config.NUM_LEGS), # limit on nb leg touching the floor
                                          np.array([0.15,0.2,0.3]),
                                          np.array([1.0]*4))) +  OBSERVATION_EPS) # limit on orientation
        observation_low = (np.concatenate((np.array([ -0.261799,  0.261799, -2.69653369433 ] * self._robot_config.NUM_LEGS), # joint limit
                                          -self._robot_config.VELOCITY_LIMITS,
                                          np.array([0]* self._robot_config.NUM_LEGS),
                                          np.array([-0.15,-0.2,-0.3]),
                                          np.array([-1.0]*4))) -  OBSERVATION_EPS)

reward = 10*vel_tracking_reward \
            + 2*yaw_reward \
            + 4*roll_reward \
            + drift_reward \
            - 0.01 * energy_reward \
            - 0.1 * np.linalg.norm(self.robot.GetBaseOrientation() - np.array([0,0,0,1]))