learning alg SAC
desired velocity: -1 m/s
gait mode: pace
motor control mode TORQUE
move reverse True

observation_high = (np.concatenate((np.array([ 0.261799,  1.5708, -0.916297857297 ] * self._robot_config.NUM_LEGS), # joint limit
                                          self._robot_config.VELOCITY_LIMITS, # limit on velocity
                                          self._robot_config.TORQUE_LIMITS,
                                          np.array([1]* self._robot_config.NUM_LEGS), # limit on nb leg touching the floor
                                          np.array([0.15,0.2,0.3]),
                                          np.array([1.0]*4))) +  OBSERVATION_EPS) # limit on orientation
        observation_low = (np.concatenate((np.array([ -0.261799,  0.261799, -2.69653369433 ] * self._robot_config.NUM_LEGS), # joint limit
                                          -self._robot_config.VELOCITY_LIMITS,
                                          -self._robot_config.TORQUE_LIMITS,
                                          np.array([3]* self._robot_config.NUM_LEGS),
                                          np.array([-0.15,-0.2,-0.3]),
                                          np.array([-1.0]*4))) -  OBSERVATION_EPS)

self._observation = np.concatenate((self.robot.GetMotorAngles(), 
                                            self.robot.GetMotorVelocities(),
                                            self.robot.GetMotorTorques(),
                                            self.robot.GetContactInfo()[3],
                                            self.robot.GetBaseOrientationRollPitchYaw(),
                                            self.robot.GetBaseOrientation() ))


 reward = 4*vel_tracking_reward \
            + 2*yaw_reward \
            + roll_reward \
            + drift_reward \
            - 0.01 * energy_reward \
            - 0.1 * np.linalg.norm(self.robot.GetBaseOrientation() - np.array([0,0,0,1]))